{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "AcceleratorState has already been initialized and cannot be changed, restart your runtime completely and pass `mixed_precision='fp16'` to `Accelerator()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m Unet(\n\u001b[0;32m      5\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m      6\u001b[0m     dim_mults \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m),\n\u001b[0;32m      7\u001b[0m     flash_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m diffusion \u001b[38;5;241m=\u001b[39m GaussianDiffusion(\n\u001b[0;32m     11\u001b[0m     model,\n\u001b[0;32m     12\u001b[0m     image_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m     13\u001b[0m     timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m,           \u001b[38;5;66;03m# number of steps\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     sampling_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m    \u001b[38;5;66;03m# number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])\u001b[39;00m\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m0_Git_Iman\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAll BGSU Projcts\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mZ. RA & Desertation\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDDPM\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCode\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCode_5 well illustrate\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdenoising-diffusion-pytorch\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdaisy2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_lr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_num_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# total training steps\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulate_every\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# gradient accumulation steps\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mema_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.995\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# exponential moving average decay\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# turn on mixed precision\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalculate_fid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# whether to calculate fid during training\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\0_Git_Iman\\All BGSU Projcts\\Z. RA & Desertation\\DDPM\\Code\\Code_5 well illustrate\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:887\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, diffusion_model, folder, train_batch_size, gradient_accumulate_every, augment_horizontal_flip, train_lr, train_num_steps, ema_update_every, ema_decay, adam_betas, save_and_sample_every, num_samples, results_folder, amp, mixed_precision_type, split_batches, convert_image_to, calculate_fid, inception_block_idx, max_grad_norm, num_fid_samples, save_best_and_latest_only)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTrainer\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[0;32m    868\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    869\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    870\u001b[0m         diffusion_model,\n\u001b[0;32m    871\u001b[0m         folder,\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    873\u001b[0m         train_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m    874\u001b[0m         gradient_accumulate_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    875\u001b[0m         augment_horizontal_flip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    876\u001b[0m         train_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m,\n\u001b[0;32m    877\u001b[0m         train_num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m,\n\u001b[0;32m    878\u001b[0m         ema_update_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m    879\u001b[0m         ema_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.995\u001b[39m,\n\u001b[0;32m    880\u001b[0m         adam_betas \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.99\u001b[39m),\n\u001b[0;32m    881\u001b[0m         save_and_sample_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;66;03m# 1000\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m,\n\u001b[0;32m    883\u001b[0m         results_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    884\u001b[0m         amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    885\u001b[0m         mixed_precision_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    886\u001b[0m         split_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m--> 887\u001b[0m         convert_image_to \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    888\u001b[0m         calculate_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    889\u001b[0m         inception_block_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m    890\u001b[0m         max_grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m,\n\u001b[0;32m    891\u001b[0m         num_fid_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m,\n\u001b[0;32m    892\u001b[0m         save_best_and_latest_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    893\u001b[0m     ):\n\u001b[0;32m    894\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m    896\u001b[0m         \u001b[38;5;66;03m# accelerator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:380\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[1;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, deepspeed_plugin, fsdp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, dispatch_batches, even_batches, use_seedable_sampler, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_recipe_handler \u001b[38;5;241m=\u001b[39m FP8RecipeKwargs()\n\u001b[0;32m    379\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m--> 380\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[43mAcceleratorState\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmixed_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamo_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamo_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeepspeed_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeepspeed_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfsdp_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsdp_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmegatron_lm_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmegatron_lm_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_from_accelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m trackers \u001b[38;5;241m=\u001b[39m filter_trackers(log_with, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogging_dir)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trackers) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m log_with \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\accelerate\\state.py:763\u001b[0m, in \u001b[0;36mAcceleratorState.__init__\u001b[1;34m(self, mixed_precision, cpu, dynamo_plugin, deepspeed_plugin, fsdp_plugin, megatron_lm_plugin, _from_accelerator, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m     PartialState(cpu, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(PartialState\u001b[38;5;241m.\u001b[39m_shared_state)\n\u001b[1;32m--> 763\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized:\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\accelerate\\state.py:863\u001b[0m, in \u001b[0;36mAcceleratorState._check_initialized\u001b[1;34m(self, mixed_precision, cpu)\u001b[0m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err\u001b[38;5;241m.\u001b[39mformat(flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu=True\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    859\u001b[0m     mixed_precision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m mixed_precision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mixed_precision\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m    862\u001b[0m ):\n\u001b[1;32m--> 863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err\u001b[38;5;241m.\u001b[39mformat(flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed_precision=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmixed_precision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: AcceleratorState has already been initialized and cannot be changed, restart your runtime completely and pass `mixed_precision='fp16'` to `Accelerator()`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n",
    "\n",
    "model = Unet(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    flash_attn = True\n",
    ")\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 128,\n",
    "    timesteps = 10000,           # number of steps\n",
    "    sampling_timesteps = 250    # number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    diffusion,\n",
    "    r'C:\\0_Git_Iman\\All BGSU Projcts\\Z. RA & Desertation\\DDPM\\Code\\Code_5 well illustrate\\denoising-diffusion-pytorch\\images\\daisy2',\n",
    "    train_batch_size = 8,\n",
    "    train_lr = 8e-5,\n",
    "    train_num_steps = 20,         # total training steps\n",
    "    gradient_accumulate_every = 2,    # gradient accumulation steps\n",
    "    ema_decay = 0.995,                # exponential moving average decay\n",
    "    amp = True,                       # turn on mixed precision\n",
    "    calculate_fid = True              # whether to calculate fid during training\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "you should have at least 100 images in your folder. at least 10k images recommended",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Unet(\n\u001b[0;32m      3\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m      4\u001b[0m     dim_mults \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m),\n\u001b[0;32m      5\u001b[0m     flash_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m diffusion \u001b[38;5;241m=\u001b[39m GaussianDiffusion(\n\u001b[0;32m      9\u001b[0m     model,\n\u001b[0;32m     10\u001b[0m     image_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m     11\u001b[0m     timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m,           \u001b[38;5;66;03m# number of steps\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     sampling_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m    \u001b[38;5;66;03m# number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])\u001b[39;00m\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiffusion_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m0_Git_Iman\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAll BGSU Projcts\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mZ. RA & Desertation\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDDPM\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCode\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCode_5 well illustrate\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdenoising-diffusion-pytorch\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdaisy20\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust to meet the effective batch size requirement\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulate_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure this parameter is defined and adjusted\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_num_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\0_Git_Iman\\All BGSU Projcts\\Z. RA & Desertation\\DDPM\\Code\\Code_5 well illustrate\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:922\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, diffusion_model, folder, train_batch_size, gradient_accumulate_every, augment_horizontal_flip, train_lr, train_num_steps, ema_update_every, ema_decay, adam_betas, save_and_sample_every, num_samples, results_folder, amp, mixed_precision_type, split_batches, convert_image_to, calculate_fid, inception_block_idx, max_grad_norm, num_fid_samples, save_best_and_latest_only)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;66;03m# dataset and dataloader\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds \u001b[38;5;241m=\u001b[39m Dataset(folder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size, augment_horizontal_flip \u001b[38;5;241m=\u001b[39m augment_horizontal_flip, convert_image_to \u001b[38;5;241m=\u001b[39m convert_image_to)\n\u001b[1;32m--> 922\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myou should have at least 100 images in your folder. at least 10k images recommended\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    924\u001b[0m dl \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds, batch_size \u001b[38;5;241m=\u001b[39m train_batch_size, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, pin_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, num_workers \u001b[38;5;241m=\u001b[39m cpu_count())\n\u001b[0;32m    926\u001b[0m dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(dl)\n",
      "\u001b[1;31mAssertionError\u001b[0m: you should have at least 100 images in your folder. at least 10k images recommended"
     ]
    }
   ],
   "source": [
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n",
    "model = Unet(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    flash_attn = True\n",
    ")\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 128,\n",
    "    timesteps = 10000,           # number of steps\n",
    "    sampling_timesteps = 250    # number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    diffusion_model=diffusion,\n",
    "    folder= r'C:\\0_Git_Iman\\All BGSU Projcts\\Z. RA & Desertation\\DDPM\\Code\\Code_5 well illustrate\\denoising-diffusion-pytorch\\images\\daisy20',\n",
    "    train_batch_size=5,  # Adjust to meet the effective batch size requirement\n",
    "    gradient_accumulate_every=4,  # Ensure this parameter is defined and adjusted\n",
    "    train_num_steps=10000\n",
    ")\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
